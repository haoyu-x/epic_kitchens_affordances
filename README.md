# Epic kitchens affordances

## Dataset creation: automatic annotations

The EPIC-Affordance dataset is a new dataset build on the Epic Kitchens 100 and Epic Kitchens VISOR, containing **automatic annotations** generated by the intersection of both datasets. We provide **50,242** images with **123,233** different annotation masks. The affordances are all the possible actions for an agent depending on its context (present objects and motor-sensing capabilities). In order to avoid the consuming manual labelling of the affordances due to its ambiguity and extension, we assume that correspond to all the past actions taken in that place and filtered by the current distribution of the objects. Therefore, we first obtain in a common reference the localization of all the past actions taken in a kitchen (steps 1 and 2). Then, we reproject back all the past interactions and we select only the actions whose respective object was present in the scene (step 3).

You can download all the data in this [link] (https://unizares-my.sharepoint.com/:f:/g/personal/lmur_unizar_es/Eh6XvqAezQNMi37G_nEJu0sBUPux3UOPO-LzWmDLQ3QkGQ?e=qeddVK)
-Images: we already provide the images extracted from the videos of EPIC-100 Kitchens. This avoids download the approximate 700 GB of that dense dataset.
-Annotations in 3D: in a pickle format, we provide a dictionary with the Colmap data (camera pose, camera intrinsics and keypoints), the distribution of the interacting objects, the annotation of the interaction and the distribution of the neutral objects. We encourage to the research community to use this data to develop new tasks like goal path planning.
-Affordance annotations in the 2D: we already run the project_from_3D_to_2D.py for all the sequences in order to provide a pickle dictionary with the location of the interaction points for the 32 different afforded-actions.

*Note:* Some sequences are P04_EPIC_55 and P04_EPIC_100, while others are just P01. Since scenes in EP-100 and EP-55 were recorded with different cameras, we make this distintion in order to avoid problems with the camera pose registration. If the sequence is called just P01, it means that all its frames correspond to EP-55 or EP-100.

### 1. Detect the spatial localization of the interaction

On one hand, we use the narration annotations of the Epic Kitchens 100 to obtain the semantics of the interaction (e.g "cut onion"). Then, we use the masks provided by EPIC VISOR to discover the location of that interaction, placed in the center of the intersection between the respective hand/glove and the interacting object. This provides an understanding about where the interaction occurs at that time step.


<p align="center" width="100%">
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P01_01_frame_0000003682.jpg"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P01_01_frame_0000019463.jpg"> 
</p>
<p align="center" width="100%">
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P01_01_frame_0000049183.jpg"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P01_01_frame_0000091442.jpg"> 
</p>
<p align="center" width="100%">
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000000946.jpg"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000005376.jpg"> 
</p>

### 2. Leverage all to the 3D

In a second stage, using Structure from Motion algorithms (COLMAP), we get the camera pose amd the global localization of the interaction in the 3D space obtaining a historical distribution of all the carried actions in that space. 
In the following images, we show in blue the different camera poses, in grey the Colmap keypoints and the different locations where the interactions occur.

<p align="center" width="100%">
    <img width="47%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/Screenshot%20from%202022-12-14%2016-28-24.png"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/Screenshot%20from%202022-12-13%2010-31-56.png"> 
</p>

### 3. Reproject the 3D to the 2D to obtain the affordances.

Using the camera intrinsic matrix and the camera pose provided in the "3D_output" directories, we reproject all the past interactions by running *"project_from_3D_to_2D.py"*. To filter the past interactions by the distribution of the objects in each scene, we use the VISOR annotations (active objects) and a constant distribution of passive objects (cupboard, oven, hob, fridge) which localization did not change with time. For example, if the VISOR annotation does not detect and "active cupboard", but in the past we opened the cupboard in that location, it means that there is a cupboard innactive. Therefore, we should detect that past interaction as a affordance, since it is a possible action associated to that 3D region.

We show some images of different affordances. Each point represents the location of a past interaction whose interacting objects are present.

<p align="center" width="100%">
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_05_frame_0000111070.png"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_12_frame_0000008119.png"> 
</p>

Finally, for a better visualization we obtain a Gaussian heatmaps for each of the afforded actions.
Takeable and insertable
<p align="center" width="100%">
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000016034%20take.png"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000033785%20insert.png"> 
</p>
Cuttable and driable
<p align="center" width="100%">
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000065888%20cut.png"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_04_frame_0000006974%20dry.png"> 
</p>

*Note*: the files in the *2D_output_labels* directories only contain the pixel points with the affordances and its semantic labels. When you run data.py, in the dataloader we incorporate a function to obtain the Gaussian heatmaps in an efficient way. This avoids to load the *N* masks.

## Baselines

We implemented different baselines, which are extensions of popular semantic segmentation datasets.
