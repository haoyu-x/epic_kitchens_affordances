# Epic-Aff Dataset

## Dataset creation: automatic annotations

The EPIC-Aff dataset is a new dataset build on the Epic Kitchens 100 and Epic Kitchens VISOR, containing **automatic annotations** generated by the intersection of both datasets. We provide **38,335** images with **multi-label** affordance annotation masks. The affordances are all the possible actions for an agent depending on its context (present objects and motor-sensing capabilities). There are two versions of the dataset, easy-EPIC Aff and complex-EPIC Aff, with 20 and 43 different classes respectively.
We provide a example sequence on the PO3_EPIC_100_Example. We will release the full dataset when acceptance.

-**Images**: we already provide the images extracted from the videos of EPIC-100 Kitchens in 480x854 of resolution. This avoids download the approximate 700 GB of that dense dataset.

-**Annotations in 3D**: in a pickle format, we provide a dictionary with the Colmap data (camera pose, camera intrinsics and keypoints), the distribution of the interacting objects, the annotation of the interaction and the distribution of the neutral objects. We encourage to the research community to use this data to develop new tasks like goal path planning.

-**Affordance annotations in the 2D**: we already run the project_from_3D_to_2D.py for all the sequences in order to provide a pickle dictionary with the location of the interaction points for the afforded-actions.

-**VISORs masks**: the semantic mask wit the active objets, which we consider dynamic.

-**Colmap VISOR masks**: a binary mask that indicates if the object is active (dynamic) or not. We use them to help Colmap to extract the camera poses.

### 1. Detect the spatial localization of the interaction

On one hand, we use the narration annotations of the Epic Kitchens 100 to obtain the semantics of the interaction (e.g "cut onion"). Then, we use the masks provided by EPIC VISOR to discover the location of that interaction, placed in the center of the intersection between the respective hand/glove and the interacting object. This provides an understanding about where the interaction occurs at that time step.


<p align="center" width="100%">
    <img width="32%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P01_01_frame_0000003682.jpg"> 
    <img width="32%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P01_01_frame_0000019463.jpg"> 
    <img width="32%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P01_01_frame_0000049183.jpg"> 
</p>
<p align="center" width="100%">
    <img width="32%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P01_01_frame_0000049183.jpg"> 
    <img width="32%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000000946.jpg"> 
    <img width="32%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000005376.jpg"> 
</p>

### 2. Leverage all to the 3D

In a second stage, using Structure from Motion algorithms (COLMAP), we get the camera pose amd the global localization of the interaction in the 3D space. This creates a historical distribution of all the taken actions in that environment, cross-linking alng different episodes. In the following images, we show in blue the different camera poses, in grey the Colmap keypoints and the different locations where the interactions occur.

<p align="center" width="100%">
    <img width="47%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/Screenshot%20from%202022-12-14%2016-28-24.png"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/Screenshot%20from%202022-12-13%2010-31-56.png"> 
</p>

### 3. Reproject the 3D to the 2D to obtain the affordances.

Using the camera intrinsic matrix and the camera pose provided in the "3D_output" directories, we reproject all the past interactions by running *"project_from_3D_to_2D.py"*. To filter the past interactions by the distribution of the objects in each scene, we use the VISOR annotations (active objects) and a constant distribution of passive objects (cupboard, oven, hob, fridge) which localization did not change with time. For example, if the VISOR annotation does not detect and "active cupboard", but in the past we opened the cupboard in that location, it means that there is a cupboard innactive. Therefore, we should detect that past interaction as a affordance, since it is a possible action associated to that 3D region.

We show some images of different affordances. Each point represents the location of a past interaction whose interacting objects are present.

<p align="center" width="100%">
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_05_frame_0000111070.png"> 
    <img width="45%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_12_frame_0000008119.png"> 
</p>

Finally, for a better visualization we obtain a Gaussian heatmaps for each of the afforded actions. We show respectively: takeable, insertable, cuttable and driable.
<p align="center" width="100%">
    <img width="24%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000016034%20take.png"> 
    <img width="24%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000033785%20insert.png"> 
    <img width="24%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_02_frame_0000065888%20cut.png"> 
    <img width="24%" src="https://github.com/lmur98/epic_kitchens_affordances/blob/main/imgs/P04_04_frame_0000006974%20dry.png"> 
</p>

*Note*: the files in the *2D_output_labels* directories only contain the pixel points with the affordances and its semantic labels. When you run data.py, in the dataloader we incorporate a function to obtain the Gaussian heatmaps in an efficient way. This avoids to load the *N* masks.

## Dataset pipeline
We also share the dataset pipeline extraction, an we hope that the research community apply it on their models.

